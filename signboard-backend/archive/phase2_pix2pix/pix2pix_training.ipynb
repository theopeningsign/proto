{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¨ ê°„íŒ Pix2Pix í•™ìŠµ ë…¸íŠ¸ë¶\n",
        "\n",
        "**ëª©ì :** Phase 1 CG ì´ë¯¸ì§€ë¥¼ ì‹¤ì œ ì‚¬ì§„ì²˜ëŸ¼ ë³€í™˜í•˜ëŠ” ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "**ë°ì´í„°:** paired_data (INPUT: CG ê°„íŒ, TARGET: ì‹¤ì œ ì‚¬ì§„)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„\n",
        "\n",
        "1. âœ… Google Driveì— ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ (`Colab_Signboard_Data` í´ë”)\n",
        "2. âœ… ì´ ë…¸íŠ¸ë¶ì„ Google Colabì—ì„œ ì—´ê¸°\n",
        "3. âœ… GPU ëŸ°íƒ€ì„ ì‚¬ìš© í™•ì¸ (ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1ï¸âƒ£ í™˜ê²½ í™•ì¸ ë° GPU ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU í™•ì¸\n",
        "import torch\n",
        "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPUê°€ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŸ°íƒ€ì„ì„ GPUë¡œ ë³€ê²½í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2ï¸âƒ£ Google Drive ì—°ê²°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ë°ì´í„° ê²½ë¡œ í™•ì¸\n",
        "import os\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab_Signboard_Data'\n",
        "\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"âœ… ë°ì´í„° ê²½ë¡œ í™•ì¸: {DATA_PATH}\")\n",
        "    # íŒŒì¼ ê°œìˆ˜ í™•ì¸\n",
        "    train_input_count = len([f for f in os.listdir(os.path.join(DATA_PATH, 'train', 'input')) if f.endswith(('.png', '.jpg'))])\n",
        "    train_target_count = len([f for f in os.listdir(os.path.join(DATA_PATH, 'train', 'target')) if f.endswith(('.png', '.jpg'))])\n",
        "    test_input_count = len([f for f in os.listdir(os.path.join(DATA_PATH, 'test', 'input')) if f.endswith(('.png', '.jpg'))])\n",
        "    test_target_count = len([f for f in os.listdir(os.path.join(DATA_PATH, 'test', 'target')) if f.endswith(('.png', '.jpg'))])\n",
        "    print(f\"\\nğŸ“Š ë°ì´í„° í†µê³„:\")\n",
        "    print(f\"  Train: {train_input_count} input, {train_target_count} target\")\n",
        "    print(f\"  Test: {test_input_count} input, {test_target_count} target\")\n",
        "else:\n",
        "    print(f\"âŒ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_PATH}\")\n",
        "    print(\"\\nğŸ“‹ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
        "    print(\"  1. Google Driveì— 'Colab_Signboard_Data' í´ë”ê°€ ìˆëŠ”ì§€\")\n",
        "    print(\"  2. í´ë” êµ¬ì¡°ê°€ ì˜¬ë°”ë¥¸ì§€ (train/input, train/target, test/input, test/target)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3ï¸âƒ£ Pix2Pix ì €ì¥ì†Œ í´ë¡  ë° ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì´ë™\n",
        "%cd /content\n",
        "\n",
        "# Pix2Pix ì €ì¥ì†Œ í´ë¡  (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)\n",
        "if not os.path.exists('pytorch-CycleGAN-and-pix2pix'):\n",
        "    !git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n",
        "    print(\"âœ… ì €ì¥ì†Œ í´ë¡  ì™„ë£Œ\")\n",
        "else:\n",
        "    print(\"âœ… ì €ì¥ì†Œê°€ ì´ë¯¸ ìˆìŠµë‹ˆë‹¤\")\n",
        "\n",
        "%cd pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install -q dominate visdom\n",
        "print(\"âœ… í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4ï¸âƒ£ ë°ì´í„°ë¥¼ Pix2Pix í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pix2Pix ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "import shutil\n",
        "\n",
        "DATASET_NAME = 'signboards'\n",
        "DATASET_DIR = f'/content/pytorch-CycleGAN-and-pix2pix/datasets/{DATASET_NAME}'\n",
        "\n",
        "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "os.makedirs(f'{DATASET_DIR}/train/A', exist_ok=True)\n",
        "os.makedirs(f'{DATASET_DIR}/train/B', exist_ok=True)\n",
        "os.makedirs(f'{DATASET_DIR}/test/A', exist_ok=True)\n",
        "os.makedirs(f'{DATASET_DIR}/test/B', exist_ok=True)\n",
        "\n",
        "# ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± (ë³µì‚¬ ëŒ€ì‹  ë§í¬ ì‚¬ìš© - ë¹ ë¦„)\n",
        "# Train\n",
        "!ln -s \"{DATA_PATH}/train/input/\"* \"{DATASET_DIR}/train/A/\" 2>/dev/null || true\n",
        "!ln -s \"{DATA_PATH}/train/target/\"* \"{DATASET_DIR}/train/B/\" 2>/dev/null || true\n",
        "\n",
        "# Test\n",
        "!ln -s \"{DATA_PATH}/test/input/\"* \"{DATASET_DIR}/test/A/\" 2>/dev/null || true\n",
        "!ln -s \"{DATA_PATH}/test/target/\"* \"{DATASET_DIR}/test/B/\" 2>/dev/null || true\n",
        "\n",
        "# í™•ì¸\n",
        "train_a_count = len([f for f in os.listdir(f'{DATASET_DIR}/train/A') if not f.startswith('.')])\n",
        "train_b_count = len([f for f in os.listdir(f'{DATASET_DIR}/train/B') if not f.startswith('.')])\n",
        "test_a_count = len([f for f in os.listdir(f'{DATASET_DIR}/test/A') if not f.startswith('.')])\n",
        "test_b_count = len([f for f in os.listdir(f'{DATASET_DIR}/test/B') if not f.startswith('.')])\n",
        "\n",
        "print(f\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:\")\n",
        "print(f\"  Train: A={train_a_count}, B={train_b_count}\")\n",
        "print(f\"  Test: A={test_a_count}, B={test_b_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5ï¸âƒ£ ë°ì´í„° ìƒ˜í”Œ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìƒ˜í”Œ ì´ë¯¸ì§€ í™•ì¸\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_sample(train_dir, sample_idx=0):\n",
        "    a_files = sorted([f for f in os.listdir(f'{train_dir}/A') if f.endswith(('.png', '.jpg')) and not f.startswith('.')])\n",
        "    b_files = sorted([f for f in os.listdir(f'{train_dir}/B') if f.endswith(('.png', '.jpg')) and not f.startswith('.')])\n",
        "    \n",
        "    if sample_idx < len(a_files) and sample_idx < len(b_files):\n",
        "        a_path = os.path.join(train_dir, 'A', a_files[sample_idx])\n",
        "        b_path = os.path.join(train_dir, 'B', b_files[sample_idx])\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "        \n",
        "        img_a = Image.open(a_path)\n",
        "        axes[0].imshow(img_a)\n",
        "        axes[0].set_title(f'INPUT (CG) - {a_files[sample_idx]}', fontsize=12)\n",
        "        axes[0].axis('off')\n",
        "        \n",
        "        img_b = Image.open(b_path)\n",
        "        axes[1].imshow(img_b)\n",
        "        axes[1].set_title(f'TARGET (ì‹¤ì œ ì‚¬ì§„) - {b_files[sample_idx]}', fontsize=12)\n",
        "        axes[1].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"âŒ ìƒ˜í”Œ ì¸ë±ìŠ¤ {sample_idx}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(\"ğŸ“¸ Train ë°ì´í„° ìƒ˜í”Œ:\")\n",
        "show_sample(f'{DATASET_DIR}/train', 0)\n",
        "\n",
        "print(\"\\nğŸ“¸ Test ë°ì´í„° ìƒ˜í”Œ:\")\n",
        "show_sample(f'{DATASET_DIR}/test', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6ï¸âƒ£ í•™ìŠµ ì‹œì‘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • (ğŸš¨ ì´ë¯¸ì§€ ë¶„í•  ë¬¸ì œ í•´ê²°!)\n",
        "MODEL_NAME = 'signboard_pix2pix_FIXED'  # ìƒˆ ëª¨ë¸ëª…\n",
        "EPOCHS = 25  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì§§ê²Œ ì„¤ì •\n",
        "EPOCHS_DECAY = 25  # Learning rate decay epoch\n",
        "BATCH_SIZE = 1  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • (1, 2, 4 ë“±)\n",
        "\n",
        "print(f\"ğŸ¯ ìˆ˜ì •ëœ í•™ìŠµ ì„¤ì •:\")\n",
        "print(f\"  ëª¨ë¸ ì´ë¦„: {MODEL_NAME}\")\n",
        "print(f\"  Epochs: {EPOCHS} + {EPOCHS_DECAY} (decay)\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  ë°ì´í„°ì…‹: {DATASET_NAME}\")\n",
        "print(f\"\\nâ±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: 30ë¶„-1ì‹œê°„ (GPU ê¸°ì¤€)\")\n",
        "print(f\"\\nğŸ’¡ ì¤‘ê°„ ê²°ê³¼ëŠ” checkpoints/{MODEL_NAME}/web/images/ ì— ì €ì¥ë©ë‹ˆë‹¤.\")\n",
        "print(f\"\\nğŸ”§ ì´ë¯¸ì§€ ë¶„í•  ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¶”ê°€ ì˜µì…˜ì´ ì ìš©ë©ë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•™ìŠµ ì‹¤í–‰ (ğŸš¨ ì´ë¯¸ì§€ ë¶„í•  ë¬¸ì œ ì™„ì „ í•´ê²°!)\n",
        "!python train.py --dataroot ./datasets/{DATASET_NAME} --name {MODEL_NAME} --model pix2pix --direction AtoB --dataset_mode aligned --load_size 512 --crop_size 512 --preprocess none --no_flip --input_nc 3 --output_nc 3 --batch_size {BATCH_SIZE} --n_epochs {EPOCHS} --n_epochs_decay {EPOCHS_DECAY} --display_freq 25 --print_freq 25 --save_epoch_freq 5\n",
        "\n",
        "print('ğŸš¨ ì´ë¯¸ì§€ ë¶„í•  ë¬¸ì œ í•´ê²° ì˜µì…˜ë“¤:')\n",
        "print('  --dataset_mode aligned: ë³„ë„ A/B í´ë” ì‚¬ìš©')\n",
        "print('  --load_size 512: ì´ë¯¸ì§€ë¥¼ 512x512ë¡œ ë¡œë“œ')\n",
        "print('  --crop_size 512: í¬ë¡­ ì—†ì´ ì „ì²´ ì´ë¯¸ì§€ ì‚¬ìš©')\n",
        "print('  --preprocess none: ì¶”ê°€ ì „ì²˜ë¦¬ ì—†ìŒ')\n",
        "print('  --no_flip: ì¢Œìš° ë°˜ì „ ë¹„í™œì„±í™” (í•µì‹¬!)')\n",
        "print('  --input_nc 3 --output_nc 3: 3ì±„ë„ ëª…ì‹œ')\n",
        "print('  --display_freq 25: 25 iterë§ˆë‹¤ ê²°ê³¼ ì €ì¥')\n",
        "print('ğŸ¯ ì™„ì „í•œ ì´ë¯¸ì§€ë¡œ í•™ìŠµí•©ë‹ˆë‹¤!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7ï¸âƒ£ ì¤‘ê°„ ê²°ê³¼ í™•ì¸ (í•™ìŠµ ì¤‘ê°„ì— ì‹¤í–‰ ê°€ëŠ¥)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš¡ ë¹ ë¥¸ í™•ì¸ (Epoch 10ì—ì„œ)\n",
        "\n",
        "**í•™ìŠµ ì¤‘ê°„ì— ì´ë¯¸ì§€ ë¶„í•  ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ì„¸ìš”!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš¨ ì¤‘ìš”! Epoch 10 ì •ë„ì—ì„œ ì´ ì…€ì„ ì‹¤í–‰í•´ì„œ ë¶„í•  ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!\n",
        "result_dir = f'/content/pytorch-CycleGAN-and-pix2pix/checkpoints/{MODEL_NAME}/web/images'\n",
        "\n",
        "if os.path.exists(result_dir):\n",
        "    # epoch010 í™•ì¸\n",
        "    epoch_dir = os.path.join(result_dir, 'epoch010')\n",
        "    if os.path.exists(epoch_dir):\n",
        "        print(\"âœ… Epoch 10 ê²°ê³¼ í™•ì¸:\")\n",
        "        \n",
        "        real_a_path = os.path.join(epoch_dir, 'real_A.png')\n",
        "        fake_b_path = os.path.join(epoch_dir, 'fake_B.png') \n",
        "        real_b_path = os.path.join(epoch_dir, 'real_B.png')\n",
        "        \n",
        "        if all(os.path.exists(p) for p in [real_a_path, fake_b_path, real_b_path]):\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "            \n",
        "            img_a = Image.open(real_a_path)\n",
        "            axes[0].imshow(img_a)\n",
        "            axes[0].set_title('INPUT (CG) - ì™„ì „í•œ ê°„íŒì´ì–´ì•¼ í•¨!', fontsize=14)\n",
        "            axes[0].axis('off')\n",
        "            print(f\"Real_A í¬ê¸°: {img_a.size}\")\n",
        "            \n",
        "            img_fake = Image.open(fake_b_path)\n",
        "            axes[1].imshow(img_fake) \n",
        "            axes[1].set_title('OUTPUT (ìƒì„±) - ì™„ì „í•œ ê°„íŒì´ì–´ì•¼ í•¨!', fontsize=14)\n",
        "            axes[1].axis('off')\n",
        "            print(f\"Fake_B í¬ê¸°: {img_fake.size}\")\n",
        "            \n",
        "            img_b = Image.open(real_b_path)\n",
        "            axes[2].imshow(img_b)\n",
        "            axes[2].set_title('TARGET (ì‹¤ì œ) - ì™„ì „í•œ ê°„íŒì´ì–´ì•¼ í•¨!', fontsize=14) \n",
        "            axes[2].axis('off')\n",
        "            print(f\"Real_B í¬ê¸°: {img_b.size}\")\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            # í¬ê¸° í™•ì¸\n",
        "            if img_a.size == (512, 512) and img_b.size == (512, 512):\n",
        "                print(\"\\nğŸ‰ ì„±ê³µ! ì´ë¯¸ì§€ê°€ ì™„ì „í•˜ê²Œ ë‚˜ì˜¤ê³  ìˆìŠµë‹ˆë‹¤!\")\n",
        "            else:\n",
        "                print(f\"\\nâŒ ë¬¸ì œ! ì´ë¯¸ì§€ í¬ê¸°ê°€ ì´ìƒí•©ë‹ˆë‹¤: A={img_a.size}, B={img_b.size}\")\n",
        "        else:\n",
        "            print(\"âŒ Epoch 10 ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(\"â³ Epoch 10ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...\")\n",
        "else:\n",
        "    print(\"âŒ ê²°ê³¼ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì´ ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê°€ì¥ ìµœê·¼ epochì˜ ê²°ê³¼ í™•ì¸\n",
        "result_dir = f'/content/pytorch-CycleGAN-and-pix2pix/checkpoints/{MODEL_NAME}/web/images'\n",
        "\n",
        "if os.path.exists(result_dir):\n",
        "    # ê°€ì¥ ìµœê·¼ epoch ì°¾ê¸° (ë””ë ‰í† ë¦¬ë§Œ, epoch + ìˆ«ì íŒ¨í„´ë§Œ)\n",
        "    import re\n",
        "    epoch_dirs = []\n",
        "    for item in os.listdir(result_dir):\n",
        "        item_path = os.path.join(result_dir, item)\n",
        "        # epochë¡œ ì‹œì‘í•˜ê³  ë””ë ‰í† ë¦¬ì´ë©°, ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ê²ƒë§Œ\n",
        "        if item.startswith('epoch') and os.path.isdir(item_path):\n",
        "            # epoch ë’¤ì— ìˆ«ìë§Œ ìˆëŠ”ì§€ í™•ì¸\n",
        "            match = re.match(r'^epoch(\\d+)$', item)\n",
        "            if match:\n",
        "                epoch_dirs.append(item)\n",
        "    \n",
        "    if epoch_dirs:\n",
        "        latest_epoch = max(epoch_dirs, key=lambda x: int(re.match(r'^epoch(\\d+)$', x).group(1)))\n",
        "        epoch_dir = os.path.join(result_dir, latest_epoch)\n",
        "        \n",
        "        # ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ\n",
        "        real_a_path = os.path.join(epoch_dir, 'real_A.png')\n",
        "        fake_b_path = os.path.join(epoch_dir, 'fake_B.png')\n",
        "        real_b_path = os.path.join(epoch_dir, 'real_B.png')\n",
        "        \n",
        "        if all(os.path.exists(p) for p in [real_a_path, fake_b_path, real_b_path]):\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "            \n",
        "            axes[0].imshow(Image.open(real_a_path))\n",
        "            axes[0].set_title('INPUT (CG)', fontsize=14)\n",
        "            axes[0].axis('off')\n",
        "            \n",
        "            axes[1].imshow(Image.open(fake_b_path))\n",
        "            axes[1].set_title(f'OUTPUT (ìƒì„±ë¨) - Epoch {latest_epoch}', fontsize=14)\n",
        "            axes[1].axis('off')\n",
        "            \n",
        "            axes[2].imshow(Image.open(real_b_path))\n",
        "            axes[2].set_title('TARGET (ì‹¤ì œ)', fontsize=14)\n",
        "            axes[2].axis('off')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"âš ï¸ {latest_epoch}ì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ ê²°ê³¼ ì´ë¯¸ì§€ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(f\"âš ï¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {result_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í•™ìŠµ ì™„ë£Œ í›„)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ğŸš¨ ì´ë¯¸ì§€ ë¶„í•  ë¬¸ì œ ì™„ì „ í•´ê²°!)\n",
        "!python test.py --dataroot ./datasets/{DATASET_NAME} --name {MODEL_NAME} --model pix2pix --direction AtoB --dataset_mode aligned --load_size 512 --crop_size 512 --preprocess none --no_flip --input_nc 3 --output_nc 3 --epoch latest\n",
        "\n",
        "print('ğŸš¨ í…ŒìŠ¤íŠ¸ ì˜µì…˜ë„ ë¶„í•  ë¬¸ì œ í•´ê²° ì ìš©!')\n",
        "print('  --no_flip: ì¢Œìš° ë°˜ì „ ë¹„í™œì„±í™”')\n",
        "print('  --input_nc 3 --output_nc 3: 3ì±„ë„ ëª…ì‹œ')\n",
        "print('ğŸ¯ ì™„ì „í•œ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9ï¸âƒ£ í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ\n",
        "test_result_dir = f'/content/pytorch-CycleGAN-and-pix2pix/results/{MODEL_NAME}/test_latest/images'\n",
        "\n",
        "if os.path.exists(test_result_dir):\n",
        "    test_files = sorted([f for f in os.listdir(test_result_dir) if f.endswith('.png')])\n",
        "    \n",
        "    # ì²« ë²ˆì§¸ ìƒ˜í”Œ í‘œì‹œ\n",
        "    if test_files:\n",
        "        real_a_files = [f for f in test_files if 'real_A' in f]\n",
        "        fake_b_files = [f for f in test_files if 'fake_B' in f]\n",
        "        real_b_files = [f for f in test_files if 'real_B' in f]\n",
        "        \n",
        "        num_samples = min(3, len(real_a_files))  # ìµœëŒ€ 3ê°œ ìƒ˜í”Œ\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "            \n",
        "            axes[0].imshow(Image.open(os.path.join(test_result_dir, real_a_files[i])))\n",
        "            axes[0].set_title('INPUT', fontsize=14)\n",
        "            axes[0].axis('off')\n",
        "            \n",
        "            axes[1].imshow(Image.open(os.path.join(test_result_dir, fake_b_files[i])))\n",
        "            axes[1].set_title('OUTPUT (ëª¨ë¸ ìƒì„±)', fontsize=14)\n",
        "            axes[1].axis('off')\n",
        "            \n",
        "            axes[2].imshow(Image.open(os.path.join(test_result_dir, real_b_files[i])))\n",
        "            axes[2].set_title('TARGET (ì‹¤ì œ)', fontsize=14)\n",
        "            axes[2].axis('off')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(\"âš ï¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(f\"âš ï¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_result_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”Ÿ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•™ìŠµëœ ëª¨ë¸ì„ Google Driveì— ì €ì¥ (ì„ íƒì‚¬í•­)\n",
        "checkpoint_dir = f'/content/pytorch-CycleGAN-and-pix2pix/checkpoints/{MODEL_NAME}'\n",
        "drive_save_dir = '/content/drive/MyDrive/Colab_Signboard_Models'\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(drive_save_dir, exist_ok=True)\n",
        "    \n",
        "    # ëª¨ë¸ íŒŒì¼ ë³µì‚¬\n",
        "    !cp -r \"{checkpoint_dir}\" \"{drive_save_dir}/\"\n",
        "    \n",
        "    print(f\"âœ… ëª¨ë¸ì´ {drive_save_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(f\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pix2pix_training",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
